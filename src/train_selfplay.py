import os
import torch
import rlcard
from rlcard.agents import RandomAgent
from src.env.wrappers import AlphaHoldemWrapper
from src.agent.ppo_agent import PPOAgent

def run_training(num_episodes=1000, print_interval=100):
    # 1. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì¤€ë¹„
    # RLCardì˜ ê¸°ë³¸ ì„¤ì •ì„ ê°€ì ¸ì˜¤ë˜, ìš°ë¦¬ê°€ ë§Œë“  Wrapperë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    raw_env = rlcard.make('no-limit-holdem', config={'seed': 42})
    env = AlphaHoldemWrapper(raw_env)

    # ìš°ë¦¬ì˜ ì£¼ì¸ê³µ AlphaHoldem (ì…ë ¥ 54, í–‰ë™ 5)
    agent = PPOAgent(input_dim=54, action_dim=5, lr=0.0002, K_epochs=4)

    # ê¸°ë¡ìš© ë³€ìˆ˜
    total_rewards = 0
    wins = 0

    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! (ì´ {num_episodes} ì—í”¼ì†Œë“œ)")

    for episode in range(1, num_episodes + 1):
        # ê²Œì„ ì‹œì‘ (í™˜ê²½ ì´ˆê¸°í™”)
        state, player_id = env.reset()
        
        # ì´ë²ˆ íŒì˜ ë°ì´í„°ë¥¼ ì„ì‹œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (Player 0, 1 ë³„ë„ ê´€ë¦¬)
        # êµ¬ì¡°: [state, action, prob]
        episode_memory = {0: [], 1: []}
        
        done = False
        
        # --- [Game Loop] ê²Œì„ì´ ëë‚  ë•Œê¹Œì§€ ì§„í–‰ ---
        while not done:
            # 1. í˜„ì¬ í”Œë ˆì´ì–´ì˜ í–‰ë™ ê²°ì •
            # (Self-Play: ë‘ í”Œë ˆì´ì–´ ëª¨ë‘ ê°™ì€ Agent ì‚¬ìš©)
            action, probs = agent.policy.get_action(state)
            
            # [ìˆ˜ì • í¬ì¸íŠ¸] ì „ì²´ í™•ë¥  í…ì„œ(probs) ëŒ€ì‹ , 
            # ì‹¤ì œë¡œ ì„ íƒí•œ í–‰ë™ì˜ í™•ë¥ ê°’(Scalar Float)ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•´ì•¼ í•¨
            prob_a = probs[0][action].item() # .item()ìœ¼ë¡œ ìˆœìˆ˜ float ë³€í™˜
            
            # 2. ì„ì‹œ ë©”ëª¨ë¦¬ì— 'ìƒíƒœ, í–‰ë™, í™•ë¥ ' ì €ì¥ (ë³´ìƒì€ ì•„ì§ ëª¨ë¦„)
            episode_memory[player_id].append({
                's': state,
                'a': action,
                'prob': prob_a
            })

            # 3. í™˜ê²½ì— í–‰ë™ ì ìš©
            next_state, next_player_id = env.step(action)
            
            # 4. ìƒíƒœ ì—…ë°ì´íŠ¸
            # (ì£¼ì˜: next_stateê°€ Noneì´ë©´ ê²Œì„ ë)
            if next_state is None:
                done = True
            else:
                state = next_state
                player_id = next_player_id

        # --- [Game Over] ê²Œì„ ì¢…ë£Œ í›„ ë³´ìƒ ì²˜ë¦¬ ---
        # RLCardì—ì„œ ìµœì¢… ìŠ¹íŒ¨ ë³´ìƒ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: [1.0, -1.0])
        payoffs = env.env.get_payoffs() 
        
        # ê° í”Œë ˆì´ì–´ì˜ ê¸°ì–µì„ ë˜ì‚´ë ¤ í•™ìŠµ ë°ì´í„°(Transition) ìƒì„±
        for pid in [0, 1]:
            reward = payoffs[pid]
            memory = episode_memory[pid]
            
            for i, step_data in enumerate(memory):
                s = step_data['s']
                a = step_data['a']
                prob = step_data['prob']
                
                # ë‹¤ìŒ ìƒíƒœ(Next State) ì •ì˜
                # í¬ì»¤ëŠ” ë‚´ í„´ -> ìƒëŒ€ í„´ -> ë‚´ í„´ ì´ë¯€ë¡œ, 
                # ë°”ë¡œ ë‹¤ìŒ ë°ì´í„°ê°€ ë‚˜ì˜ Next Stateê°€ ë¨ (ë‹¨, ë§ˆì§€ë§‰ í„´ì€ ì¢…ë£Œ ìƒíƒœ)
                if i < len(memory) - 1:
                    ns = memory[i+1]['s']
                    d = False
                else:
                    ns = s # ë§ˆì§€ë§‰ ìƒíƒœëŠ” í° ì˜ë¯¸ ì—†ìŒ (done=Trueë¼ ë¬´ì‹œë¨)
                    d = True
                
                # PPO ì—ì´ì „íŠ¸ì— ë°ì´í„° ì£¼ì…
                # (ì¤‘ìš”: PPOëŠ” Stepë³„ ë³´ìƒë³´ë‹¤, ê²Œì„ ì¢…ë£Œ ë³´ìƒì„ ì£¼ë¡œ ì‚¬ìš©)
                # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ìŠ¤í…ì—ë§Œ í° ë³´ìƒì„ ì£¼ê³  ë‚˜ë¨¸ì§„ 0 ì²˜ë¦¬ ê°€ëŠ¥í•˜ì§€ë§Œ,
                # ìš°ì„  ëª¨ë“  ìŠ¤í…ì— ìµœì¢… ë³´ìƒì„ í• ë‹¹ (Monte Carlo ë°©ì‹)
                agent.put_data((s, a, reward, ns, d, prob))

        # ê¸°ë¡ ì—…ë°ì´íŠ¸ (Player 0 ê¸°ì¤€)
        total_rewards += payoffs[0]
        if payoffs[0] > 0: wins += 1

        # --- [Training] ì¼ì • ë°ì´í„°ê°€ ëª¨ì´ë©´ í•™ìŠµ ìˆ˜í–‰ ---
        # ì—í”¼ì†Œë“œë§ˆë‹¤ ë°”ë¡œ í•™ìŠµí•˜ê±°ë‚˜, ë°°ì¹˜ë¥¼ ëª¨ì•„ì„œ í•  ìˆ˜ ìˆìŒ.
        # ì—¬ê¸°ì„œëŠ” 32 ì—í”¼ì†Œë“œë§ˆë‹¤ í•™ìŠµ ì§„í–‰
        if len(agent.data) >= 200: # ì•½ 3~4ê²Œì„ ë¶„ëŸ‰ì˜ í„´ ë°ì´í„°
            loss = agent.train_net()

        # --- [Logging] ì§„í–‰ ìƒí™© ì¶œë ¥ ---
        if episode % print_interval == 0:
            avg_reward = total_rewards / print_interval
            win_rate = wins / print_interval * 100
            print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, Win Rate (P0) = {win_rate:.1f}%")
            total_rewards = 0
            wins = 0

    # í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
    save_path = "alpha_holdem_day6.pth"
    torch.save(agent.policy.state_dict(), save_path)
    print(f"âœ… í•™ìŠµ ì¢…ë£Œ! ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    run_training()