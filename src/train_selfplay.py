import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import rlcard
from rlcard.agents import RandomAgent
from src.env.wrappers import AlphaHoldemWrapper
from src.agent.ppo_agent import PPOAgent


def evaluate(agent, env, num_games=20):
    """
    í˜„ì¬ AI(Player 0) vs ëœë¤ ë´‡(Player 1) í‰ê°€ì „
    """
    wins = 0
    total_rewards = 0
    
    for _ in range(num_games):
        state, player_id = env.reset()
        done = False
        
        while not done:
            # Player 0: ìš°ë¦¬ AI (PPO)
            if player_id == 0:
                # Deterministic=True: ì‹œí—˜ ì¹  ë•ŒëŠ” ëª¨í—˜í•˜ì§€ ì•Šê³  ìµœì„ ì˜ ìˆ˜ë§Œ ë‘ 
                action, _ = agent.policy.get_action(state, deterministic=True)
            
            # Player 1: ëœë¤ ë´‡ (Random)
            else:
                # Wrapperê°€ ì²˜ë¦¬í•´ì£¼ë¯€ë¡œ ëœë¤í•˜ê²Œ ì •ìˆ˜(0~4)ë§Œ ë½‘ìœ¼ë©´ ë¨
                # ë‹¨, í•©ë²•ì ì¸ ì•¡ì…˜ ì¤‘ì—ì„œ ê³¨ë¼ì•¼ í•¨
                # RLCardì˜ raw stateì—ì„œ legal_actions ê°€ì ¸ì˜¤ê¸°
                raw_state = env.env.get_state(player_id)
                legal_actions = list(raw_state['legal_actions'].keys())
                # ëœë¤ ì„ íƒ (Wrapperê°€ 0~4 ë§¤í•‘ ì²˜ë¦¬í•˜ë¯€ë¡œ ì´ ì¸ë±ìŠ¤ê°€ ì¤‘ìš”)
                # í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” Wrapperì˜ step(action_idx)ë¥¼ ë¶€ë¥´ë¯€ë¡œ
                # Wrapperì˜ decode ë¡œì§ì— ë§ì¶°ì•¼ í•¨.
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ: ëœë¤ ë´‡ì€ ê·¸ëƒ¥ ì•„ë¬´ê±°ë‚˜ ë˜ì§€ê³  Wrapperê°€ ì²˜ë¦¬í•˜ê²Œ ë‘ 
                # (ë” ì •í™•íˆëŠ” legal_actions ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ ì„ íƒ)
                action = np.random.choice(legal_actions)

            next_state, next_player_id = env.step(action)
            
            if next_state is None: # ê²Œì„ ì¢…ë£Œ
                done = True
                payoffs = env.env.get_payoffs()
                total_rewards += payoffs[0]
                if payoffs[0] > 0:
                    wins += 1
            else:
                state = next_state
                player_id = next_player_id
                
    return wins / num_games * 100, total_rewards / num_games

def run_training(num_episodes=5000, eval_interval=100):
    # 1. TensorBoard ì„¤ì •
    writer = SummaryWriter("runs/AlphaHoldem_Day7")
    
    # 2. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„¤ì •
    raw_env = rlcard.make('no-limit-holdem', config={'seed': 42})
    env = AlphaHoldemWrapper(raw_env)
    
    # ì•¡ì…˜ 5ê°œë¡œ í†µì¼ëœ ì„¤ì •
    agent = PPOAgent(input_dim=54, action_dim=5, lr=0.0002, K_epochs=4, eps_clip=0.2)
    
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! (ì´ {num_episodes} ì—í”¼ì†Œë“œ, í…ì„œë³´ë“œë¡œ ëª¨ë‹ˆí„°ë§ ì¤‘...)")

    # í•™ìŠµ ë£¨í”„
    for episode in range(1, num_episodes + 1):
        state, player_id = env.reset()
        episode_memory = {0: [], 1: []}
        done = False
        
        while not done:
            # Self-Play: í•­ìƒ AIê°€ í–‰ë™ ê²°ì •
            action, probs = agent.policy.get_action(state)
            
            # [Day 6 ìˆ˜ì •ì‚¬í•­ ë°˜ì˜] í™•ë¥ ê°’(Scalar)ë§Œ ì €ì¥
            action_prob = probs[0][action].item()
            
            episode_memory[player_id].append({
                's': state, 'a': action, 'prob': action_prob
            })

            next_state, next_player_id = env.step(action)
            
            if next_state is None:
                done = True
            else:
                state = next_state
                player_id = next_player_id

        # ê²Œì„ ì¢…ë£Œ ë° ë°ì´í„° ì €ì¥
        payoffs = env.env.get_payoffs()
        for pid in [0, 1]:
            reward = payoffs[pid]
            memory = episode_memory[pid]
            for i, step_data in enumerate(memory):
                s, a, prob = step_data['s'], step_data['a'], step_data['prob']
                ns = memory[i+1]['s'] if i < len(memory)-1 else s
                d = False if i < len(memory)-1 else True
                agent.put_data((s, a, reward, ns, d, prob))

        # --- [Training] í•™ìŠµ ë° Loss ê¸°ë¡ ---
        if len(agent.data) >= 256: # ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì°¨ë©´ í•™ìŠµ
            loss = agent.train_net()
            writer.add_scalar("Training/Loss", loss, episode)

        # --- [Evaluation] ì£¼ê¸°ì  í‰ê°€ ---
        if episode % eval_interval == 0:
            # ëœë¤ ë´‡ê³¼ 50íŒ ëŒ€ê²°
            win_rate, avg_reward = evaluate(agent, env, num_games=200)
            
            print(f"Episode {episode}: Eval WinRate = {win_rate:.1f}% | AvgReward = {avg_reward:.2f}")
            
            # í…ì„œë³´ë“œì— ê¸°ë¡
            writer.add_scalar("Evaluation/WinRate_vs_Random", win_rate, episode)
            writer.add_scalar("Evaluation/AvgReward_vs_Random", avg_reward, episode)
            
            # ëª¨ë¸ ì €ì¥
            torch.save(agent.policy.state_dict(), "alpha_holdem_day7.pth")

    writer.close()
    print("âœ… í•™ìŠµ ì¢…ë£Œ!")

if __name__ == "__main__":
    run_training()